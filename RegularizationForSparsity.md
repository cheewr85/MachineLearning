## 희소성을 위한 정규화
- 특성 교차로 돌아가기
  - 희소 특성 교차는 특성 공간을 크게 늘릴 수 있음
  - 가능한 문제
    - 모델 크기(RAM)가 매우 커질 수 있음
    - '노이즈'계수(과적합의 원인)
- L1 정규화
  - L0 가중치 기준에 페널티를 주고자 함
    - 볼록하지 않은 최적화,NP-난해
  - L1 정규화로의 완화:
    - 절대값(가중치)의 합에 페널티를 줌
    - 볼록 문제
    - L2와는 달리 희소성을 유도
    
## L1 정규화
- 희소 벡터는 종종 많은 차원을 포함함/이러한 고차원 특성 벡터가 주어지면 모델 크기가 커질 수 있고 엄청난 양의 RAM이 필요함
- 고차원의 희소 벡터에서는 가중치가 정확하게 0으로 떨어지도록 유도하는 것이 좋음
- 가중치가 정확하게 0일 경우 모델에서 해당 특성을 삭제함/특성을 없애면 RAM이 절약되고 모델의 노이즈가 줄어들 수 있음 
- 적절히 선택한 정규화 항을 추가함으로써, 학습 시 수행한 최적화 문제에 아이디어 적용 가능 
- L2 정규화는 가중치를 작은 값으로 유도하지만 정확히 0.0으로 만들지는 못함 
- L0 정규화는 볼록 최적화 문제를 NP-난해임 볼록하지 않은 최적화 문제로 바꿔버리는 단점이 있음/효과적으로 사용할 수 없음 
- L1 정규화를 사용하여 모델에서 유용하지 않은 많은 계수를 정확히 0이 되도록 유도하여 추론 단계에서 RAM을 절약할 수 있음 
### L1정규화와 L2정규화 비교
- L2와 L1은 서로 다른 방식으로 가중치에 페널티를 줌 
  - L2는 가중치^2에 페널티를 줌
  - L1은 |가중치|에 페널티를 줌
- 결과적으로 L2와 L1은 서로 다르게 미분됨 
  - L2의 미분계수는 2*가중치임
  - L1의 미분계수는 K(가중치와 무관한 값을 갖는 상수)임 
- L2의 미분계수는 매번 가중치의 x%만큼 제거한다고 생각하면 됨/무한히 제거해도 그 값은 절대 0이 되지 않음/가중치를 0으로 유도하지 않음
- L1의 미분계수는 매번 가중치에서 일정 상수를 빼는 것으로 생각하면 됨/L1은 0에서 불연속성을 가지며 이로 인해 0을 지나는 빼기 결과값은 0이 됨
- L1을 통해 가중치가 제거됨/모든 가중치의 절대값에 페널티를 줌
<img src="https://user-images.githubusercontent.com/32586985/70844509-5e4fe780-1e85-11ea-82ed-b90f7c66fab9.PNG">


## 실습
- L1 정규화 검사
  - 작업1:L2 정규화/정규화율(람다):0.1
  <img src="https://user-images.githubusercontent.com/32586985/70844781-29de2a80-1e89-11ea-969d-962e56d52886.PNG">
  
  - 작업2:L2 정규화/정규화율(람다):0.3
  <img src="https://user-images.githubusercontent.com/32586985/70844786-411d1800-1e89-11ea-95ea-088dbf11a8ea.PNG">
  
  - 작업3:L1 정규화/정규화율(람다):0.1
  <img src="https://user-images.githubusercontent.com/32586985/70844789-55f9ab80-1e89-11ea-9c51-c1e63dc0a9c5.PNG">
  
  - 작업4:L1 정규화/정규화율(람다):0.3
  <img src="https://user-images.githubusercontent.com/32586985/70844797-732e7a00-1e89-11ea-8fd3-c9400977dfa5.PNG">
  
  - 작업5:L1 정규화/정규화율(람다):3(실험)
  <img src="https://user-images.githubusercontent.com/32586985/70844801-7fb2d280-1e89-11ea-83e4-d5741f2b5f49.PNG">
  
  - Q1.L2에서 L1으로 정규화를 전환하면 테스트 손실과 학습 손실 사이의 델타에 어떤 영향을 주는가?
    - L2에서 L1으로 정규화를 전환하면 테스트 손실과 학습 손실 사이의 델타가 대폭 줄어듬
  - Q2.L2에서 L1으로 정규화를 전환하면 학습된 가중치에 어떤 영향을 주는가?
    - L2에서 L1으로 정규화를 전환하면 학습된 모든 가중치를 완화함
  - Q3.L1정규화율(람다)을 높이면 학습된 가중치에 어떤 영향을 주는가?
    - L1정규화율을 높이면 일반적으로 학습된 가중치가 완화되지만, 정규화율이 지나치게 높아지면 모델이 수렴할 수 없고 손실도 굉장히 높아짐 
