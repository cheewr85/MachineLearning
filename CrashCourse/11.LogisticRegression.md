## 로지스틱 회귀
- 정확히 0또는 1을 예측하는 대신 확률(0과 1사이의 값, 0과 1은 제외)을 생성함 
  - 동전 던지기의 결과를 예측하기
    - 구부러진 동전의 앞면이 나올 확률을 예측하는 문제를 생각
    - 구부러진 각도, 동전의 질량 등의 특성을 사용할 수 있음 
    - 사용할 수 있는 가장 단순한 모델은 어떤 모델인가?
    - 어떤 문제가 발생할 수 있나?
  - 로지스틱 회귀
    - 많은 문제에 확률 추정치가 출력으로 필요함
    - 로지스틱 회귀를 입력함 
    - 확률 추정치가 보정되므로 편리함 
      - 예를 들어 p(주택 판매)*가격=예상 결과
    - 이진 분류가 필요한 경우에도 유용함 
      - 스팸인가요,스팸이 아닌가요? -> p(스팸)

### 로지스틱 회귀:확률 계산
- 로지스틱 회귀는 매우 효율적인 확률 계산 매커니즘임 
- 실제로 반환된 확률을 다음 두 방법 중 하나로 사용할 수 있음 
  - 있는 그대로
    - 예시)한밤중에 개가 짖는 확률을 예측하기 위한 로지스틱 회귀 모델 
    - p(bark | night) 다음과 같은 확률을 가짐 
    - p(bark | night)가 0.05이면 개 주인은 1년 동안 18번 놀라서 깨게 됨 
    - startled = p(bark | night) * nights
    - 18 ~= 0.05 * 365
  - 이진 카테고리로 변환
    - 이진 분류 문제의 해결 방법으로 매핑함 
    - 이진 분류 문제의 목표는 가능한 두 라벨(예:'스팸' 또는 '스팸아님') 중 하나를 올바로 예측하는 것임 
- 다음과 같이 정의된 시그모이드 함수가 동일한 특성을 갖는 출력을 생성함 
<img src="https://user-images.githubusercontent.com/32586985/70524254-1e88b780-1b88-11ea-95a6-e9d0d1fec30a.PNG">
<img src="https://user-images.githubusercontent.com/32586985/70524278-2a747980-1b88-11ea-99fb-eb19e1d82dcf.PNG">

- z가 로지스틱 회귀를 사용하여 학습된 모델의 선형 레이어의 출력을 나타내는 경우 sigmoid(z)는 0과 1사이의 값(확률)을 생성함 
- 수학적 표현은 다음과 같음 
<img src="https://user-images.githubusercontent.com/32586985/70524386-5e4f9f00-1b88-11ea-9789-810577b0a941.PNG">

- y'는 특정 예에 관한 로지스틱 회귀 모델의 출력임 
- z=b+w1x1+w2x2+...wnxn
  - w값은 모델의 학습된 가중치이고,b는 편향임
  - x값은 특정 예에 대한 특성 값임 
- z는 z를 '1'라벨(예:'개가 짖음')의 확률을 '0'라벨(예:'개가 짖지 않음')의 확률로 나눈 값의 로그로 정의할 수 있는 시그모이드 상태의 역수
- 로그 오즈(log-odds)라고 함 
<img src="https://user-images.githubusercontent.com/32586985/70524569-cb633480-1b88-11ea-96d4-44d367ebd913.PNG">

- ML라벨이 포함된 시그모이드 함수
<img src="https://user-images.githubusercontent.com/32586985/70524574-ce5e2500-1b88-11ea-81d5-dbd97c656fd4.PNG">

- 예시
  - 다음과 같은 편향과 가중치를 학습한 특성이 세 개인 로지스틱 회귀 모델이 있다고 가정
    - b=1
    - w1=2
    - w2=-1
    - w3=5
  - 지정된 예의 특성 값이 다음과 같다고 가정
    - x1=0
    - x2=10
    - x3=2
  - 따라서 로그오즈는 b+w1x1+w2x2+w3x3이며 다음과 같음/(1) + (2)(0) + (-1)(10) +(5)(2) = 1
  - 결과적으로
  <img src="https://user-images.githubusercontent.com/32586985/70524775-3ca2e780-1b89-11ea-81aa-dc7094212060.PNG">
  
  ### 로지스틱 회귀:모델 학습 
  - 로지스틱 회귀의 손실 함수
  - 선형 회귀의 손실 함수는 제곱 손실임/로지스틱 회귀의 손실 함수는 로그 손실로 다음과 같이 정의됨
    - (x,y)∈D는 라벨이 있는 예(x,y쌍)가 많이 포함된 데이터 세트임 
    - y는 라벨이 있는 예의 라벨임/로지스틱 회귀이므로 y값은 모두 0 또는 1이어야 함 
    - y'는 x의 특성 세트에 대한 예측 값(0~1 사이의 값)임 
  - 로그 손실 방정식은 섀넌의 엔트로피 측정과 밀접한 관련이 있음
    - 엔트로피 측정/정보 엔트로피는 각 메시지에 포함된 정보의 기댓값이다
    - 모든 발생가능한 결과의 평균적인 정보
    - 확률이 낮을수록 어떤 정보일지도 불확실하게 되고, 이때 정보가 많다,엔트로피가 높다고 표현
    - 어떤 결과값의 발생 가능도가 작아질수록 그 정보량은 커지고, 더 자주 발생할수록 그 정보량은 작아짐
  - 우도 함수의 음의 로그로 y의 베르누이 분포를 가정함 
    - 우도 함수는 어떤 확률변수에서 표집한 값들을 토대로 그 확률변수의 모수를 구하는 방법임 
    - 베르누이 분포는 매 시행마다 오직 두 가지의 가능한 결과만 일어난다고 할 때
    - 이러한 실험을 1회 시행하여 일어난 두 가지 결과에 의해 그 값이 각각 0과 1로 결정되는 확률변수 x에 대해서
    - P(X=0)=p,P(X=1)=q,0<=p<=1,q=1-p를 만족하는 확률변수 X가 따르는 확률분포를 의미함 
  - 실제로 손실 함수를 최소화하면 최대 우도 추정치가 생성됨   
  <img src="https://user-images.githubusercontent.com/32586985/70524841-678d3b80-1b89-11ea-945a-4ec0e7551072.PNG">
  <img src="https://user-images.githubusercontent.com/32586985/70524245-19c40380-1b88-11ea-915c-c0acfb92f7b3.PNG">
  
### 로지스틱 회귀의 정규화
- 정규화는 로지스틱 회귀 모델링에서 매우 중요함/정규화하지 않으면 로지스틱 회귀의 점근 특성이 고차원에서 계속 손실을 0으로 만들려고 시도함
- 점근선을 기억해야함
- 결과적으로 모델 복잡성을 줄이기 위해 다음 두 전략 중 하나를 사용함
  - L2정규화(L2가중치감소):아주 큰 가중치에 패널티를 줌 
  - 조기중단, 즉 학습 단계 수 또는 학습률을 제한함 
- 선형 로지스틱 회귀
  - 선형 로지스틱 회귀는 매우 효율적임 
  - 학습 및 예측시간이 매우 빠름 
  - 짧고 뚱뚱한 모델이 RAM을 많이 사용함 
