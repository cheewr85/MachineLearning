## 합성 특성과 이상점 
- 다른 두 특성의 비율로 합성 특성을 만듬
- 새 특성을 선형 회귀 모델의 입력으로 사용함 
- 입력 데이터에서 이상점을 식별 및 삭제하여 모델의 효율성을 개선함 

### 설정 
- 앞에서 사용한 모델을 다시 살펴봄
- 캘리포니아 주택 데이터를 pandas DataFrame으로 가져옴 
```python
   from __future__ import print_function
   
   import math
   
   from IPython import display
   from matplotlib import cm
   from matplotlib import gridspec
   import matplotlib.pyplot as plt
   import numpy as np
   import pandas as pd
   import sklearn.metrics as metrics
   %tensorflow_version 1.x
   import tensorflow as tf
   from tensorflow.python.data import Dataset
   
   tf.logging.set_verbosity(tf.logging.ERROR)
   pd.options.display.max_rows = 10
   pd.options.display.float_format = '{:.1f}'.format
   
   california_housing_dataframe = pd.read.csv("https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv", sep=",")
   
   california_housing_dataframe = california_housing_dataframe.reindex(
       np.random.permutation(california_housing_dataframe.index))
   california_housing_dataframe["median_house_value"] /= 1000.0
   california_housing_dataframe
```
<img src="https://user-images.githubusercontent.com/32586985/69005549-02b63b00-0967-11ea-85a8-c38ebac7382f.PNG">

- 입력 함수 설정 
```python
   def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):
       """Trains a linear regression model of one feature.
       
       Args:
         features: pandas DataFrame of features
         targets: pandas DataFrame of targets
         batch_size: Size of batches to be passed to the model
         shuffle: True or False. Whether to shuffle the data.
         num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely
       Returns:
         Tuple of (features, labels) for next data batch
       """
   
   # Convert pandas data into a dict of np arrays.
   features = {key:np.array(value) for key,value in dict(features).items()}
   
   # Construct a dataset, and configure batching/repeating.
   ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit
   ds = ds.batch(batch_size).repeat(num_epochs)
   
   # Shuffle the data, if specified.
   if shuffle:
     ds = ds.shuffle(buffer_size=10000)
     
   # Return the next batch of data.
   features, labels = ds.make_one_shot_iterator().get_next()
   return features, labels
```
- 모델 학습용 함수 정의
```python
   def train_model(learning_rate, steps, batch_size, input_feature):
     """Trains a linear regression model.
     
     Args:
       learning_rate: A 'float', the learning rate.
       steps: A non-zero 'int', the total number of training steps. A training step
         consists of a forward and backward pass using a single batch.
       batch_size: A non-zero 'int', the batch size.
       input_feature: A 'string' specifying a column from 'california_housing_dataframe'
         to use as input feature.
         
     Returns:
       A Pandas 'DataFrame' containing targets and the corresponding predictions done
       after training the model
     """
     
     periods = 10
     steps_per_period = steps / periods
     
     my_feature = input_feature
     my_feature_data = california_housing_dataframe[[my_feature]].astype('float32')
     my_label = "median_house_value"
     targets = california_housing_dataframe[my_label].astype('float32')
     
     # Create input functions.
     training_input_fn = lambda: my_input_fn(my_feature_data, targets, batch_size=batch_size)
     predict_training_input_fn = lambda: my_input_fn(my_feature_data, targets, num_epochs=1, shuffle=False)
     
     # Create feature columns.
     feature_columns = [tf.feature_column.numeric_column(my_feature)]
     
     # Create a linear regressor object.
     my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)
     my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)
     linear_regressor = tf.estimator.LinearRegressor(
         feature_columns=feature_columns,
         optimizer=my_optimizer
     )
     
     # Set up to plot the state of our model's line each period.
     plt.figure(figsize=(15, 6))
     plt.subplot(1,2,1)
     plt.title("Learned Line by Period")
     plt.ylabel(my_label)
     plt.xlabel(my_feature)
     sample = california_housing_dataframe.sample(n=300)
     plt.scatter(sample[my_feature], sample[my_label])
     colors = [cm.coolwarm(x) for x in np.linspace(-1, 1, periods)]
     
     # Train the model, but do so inside a loop so that we can periodically assess
     # loss metrics.
     print("Training model...")
     print("RMSE (on training data):")
     root_mean_squared_errors = []
     for period in range (0, periods):
       # Train the model, starting from the prior state.
       linear_regressor.train(
           input_fn=training_input_fn,
           steps=steps_per_period,
       )
       # Take a break and compute predictions.
       predictions = linear_regressor.predict(input_fn=predict_training_input_fn)
       predictions = np.array([item['predictions'][0] for item in predictions])
       
       # Compute loss.
       root_mean_squared_error = math.sqrt(
         metrics.mean_squared_error(predictions, targets))
       # Occasionally print the current loss.
       print("  period %02d : %0.2f" % (period, root_mean_squared_error))
       # Add the loss metrics from this period to our list.
       root_mean_squared_error.append(root_mean_squared_error)
       # Finally, track the weights and biases over time.
       # Apply some math to ensure that the data and line are plotted neatly.
       y_extents = np.array([0, sample[my_label].max()])
       
       weight = linear_regressor.get_variable_value('linear/linear_model/%s/weights' % input_feature)[0]
       bias = linear_regressor.get_variable_value('linear/linear_model/bias_weights')
       
       x_extents = (y_extents - bias) / weight
       x_extents = np.maximum(np.minimum(x_extents,
                                         sample[my_feature].max()),
                              sample[my_feature].min())
       y_extents = weight * x_extents + bias 
       plt.plot(x_extents, y_extents, color=colors[period])
     print("Model training finished.")
     
     # Output a graph of loss metrics over periods.
     plt.subplot(1, 2, 2)
     plt.ylabel('RMSE')
     plt.xlabel('Periods')
     plt.title("Root Mean Squared Error vs. Periods")
     plt.tight_layout()
     plt.plot(root_mean_squared_errors)
     
     # Create a table with calibration data.
     calibration_data = pd.DataFrame()
     calibration_data["predictions"] = pd.Series(predictions)
     calibration_data["targets"] = pd.Series(targets)
     display.display(calibration_data.describe())
     
     print("Final RMSE (on training data): %0.2f" % root_mean_squared_error)
     
     return calibration_data                                         
```


### 작업 1: 합성 특성 사용해 보기
- total_rooms 특성과 population 특성은 모두 특정 지역의 합계를 계수함
- total_rooms와 population의 비율로 합성 특성을 만들면 지역의 인구밀도와 주택 가격 중앙값의 관계를 살펴 볼 수 있음 
- rooms_per_person이라는 특성을 만들고 train_model()의 input_feature로 사용함 
- 학습률을 조정하여 단일 특성의 성능을 얼마나 높일 수 있을까?
    - 성능이 높다는 것은 회귀선이 데이터에 잘 부합하고 최종 RMSE가 낮다는 의미
```python
   california_housing_dataframe["rooms_per_person"] = (
       california_housing_dataframe["total_rooms"] / california_housing_dataframe["population"])
   
   calibration_data = train_model(
       learning_rate=0.05,
       steps=500,
       batch_size=5,
       input_feature="rooms_per_person")
```
<img src="https://user-images.githubusercontent.com/32586985/69006088-c76b3a80-096d-11ea-8da3-a674f1702fb4.PNG">
<img src="https://user-images.githubusercontent.com/32586985/69006091-ca662b00-096d-11ea-9e87-f380a9c98b79.PNG">
<img src="https://user-images.githubusercontent.com/32586985/69006094-cd611b80-096d-11ea-93a4-ccec74fbd3f6.PNG">




### 작업 2: 이상점 식별 
- 예측과 목표값을 비교한 산포도를 작성하면 모델의 성능을 시각화할 수 있음 
- 이상적인 상태는 완벽한 상관성을 갖는 대각선이 그려지는 것 
- 작업 1에서 학습한 rooms-per-person 모델을 사용한 예측과 타겟에 대해 Pyplot의 scatter()의 산포도를 작성함 
    - rooms_per_person의 값 분포를 조사하여 소스 데이터를 추적해 보시오
```python
   plt.figure(figsize=(15, 6))
   plt.subplot(1, 2, 1)
   plt.scatter(calibration_data["predictions"], calibration_data["targets"])   
```
- 대부분의 산포점이 직선을 이룸/선이 수직에 가까움
- 선에서 벗어난 점은 비교적 적은 편임 
<img src="https://user-images.githubusercontent.com/32586985/69006145-7c9df280-096e-11ea-94b3-8f738dc3ecbe.PNG">

- 히스토그램을 그려보면 입력 데이터에서 몇 개의 이상점을 발견할 수 있음 
```python
   plt.subplot(1, 2, 2)
   _ = california_housing_dataframe["rooms_per_person"].hist()
```
<img src="https://user-images.githubusercontent.com/32586985/69006168-b1aa4500-096e-11ea-8684-9ef0d6805f7a.PNG">



### 작업 3:이상점 삭제
- rooms_per_person의 이상점 값을 적당한 최소값 또는 최대값으로 설정하여 모델의 적합성을 높일 수 있는지 확인 
```python
   clipped_feature = my_dataframe["my_feature_name"].apply(lambda x: max(x, 0))
   # Pandas Series에 함수를 적용하는 방법을 보여주는 예시/0 미만의 값을 포함하지 않음 
```

```python
   california_housing_dataframe["rooms_per_person"] = (
       california_housing_dataframe["rooms_per_person"]).apply(lambda x: min(x, 5))
       
   _ = california_housing_dataframe["rooms_per_person"].hist()  
   # 작업 2에서 작성한 히스토그램을 보면 대부분 값이 5미만임
   # rooms_per_person을 5에서 잘라내고 히스토그램 작성
```
<img src="https://user-images.githubusercontent.com/32586985/69006235-968c0500-096f-11ea-87ce-ae288e9b923b.PNG">

- 삭제가 효과 있었는지 학습을 다시 실행하고 보정 데이터를 출력해봄
```python
   calibration_data = train_model(
       learning_rate=0.05,
       steps=500,
       batch_size=5,
       input_feature="rooms_per_person")
```
- 산포도가 확실히 퍼져 있음을 볼 수 있음/데이터를 보기 좀 더 나아짐
<img src="https://user-images.githubusercontent.com/32586985/69006262-dbb03700-096f-11ea-8b0e-72f94e8beebd.PNG">
<img src="https://user-images.githubusercontent.com/32586985/69006265-deab2780-096f-11ea-9775-d56b150a7585.PNG">
<img src="https://user-images.githubusercontent.com/32586985/69006266-e1a61800-096f-11ea-9a43-d34248802893.PNG">

```python
   _ = plt.scatter(calibration_data["predictions"], calibration_data["targets"])
```
- 작업 2의 수직의 가까운 형태의 막대와는 다르게 전체적으로 분포됨         
<img src="https://user-images.githubusercontent.com/32586985/69006283-3e093780-0970-11ea-9b6c-b2ad251da6a1.PNG">




