## 신경망 학습
- 역전파는 신경망의 가장 일반적인 학습 알고리즘임
- 다계층 신경망에서 경사하강법을 사용하려면 이 알고리즘이 필요함
- 다음 사항을 봐야함
  - 데이터가 그래프를 통과하는 방식
  - 동적 프로그래밍을 사용하면 기하급수적으로 증가하는 그래프 통과 경로를 일일이 계산할 필요가 없는 이유
  - 동적 프로그래밍은 정방향 및 역방향 전달에서 중간 결과를 기록함을 의미

### 역전파:숙지할 사항
- 경사의 중요성
  - 미분 가능하면 학습이 가능할 확률이 높음
- 경사의 소실 가능성
  - 레이어를 추가할수록 신호와 노이즈가 연속적으로 감소할 수 있음
  - ReLU의 유용성
- 경사의 발산 가능성
  - 학습률의 중요성
  - batch 정규화(유용한 노브)로 해결 가능
- ReLU 레이어의 소멸 가능성
  - 당황하지 말고 학습률 낮추기 
### 특성 값 정규화
- 특성에 합리적인 척도를 부여해야함
  - 0에 대략적인 중심을 둔 [-1,1] 범위가 일반적으로 유리함
  - 경사하강법이 더 빠르게 수렴되고 NaN 트랩이 방지됨
  - 이상점 값을 배제하는 방법도 도움이 됨 
- 몇 가지 표준 방법 사용 가능
  - 선형 조정
  - 최대값, 최소값 강제 제한(클리핑)
  - 로그 조정 
### 드롭아웃 정규화
- 드롭아웃:또 하나의 정규화 형태, NN에 유용
- 단일 경사 스텝에서 네트워크의 유닛을 무작위로 배제
  - 앙상블 모델과의 접점
- 드롭아웃이 많을수록 정규화가 강력해짐
  - 0.0 = 드롭아웃 정규화 없음
  - 1.0 = 전체 드롭아웃. 학습 중지
  - 중간 범위의 값이 유용함 

## 신경망 학습:권장사항
- 실패 사례
  - 몇 가지 일반적인 이유로 인해 역전파에서 문제가 나타날 수 있음 
  - 경사 소실
    - 입력 쪽에 가까운 하위 레이어의 경사가 매우 작아질 수 있음
    - 심층 네트워크에서 이러한 경사를 계산할 때는 많은 작은 항의 곱을 구하는 과정을 포함할 수 있음 
    - 하위 레이어의 경사가 0에 가깝게 소실되면 이러한 레이어에서 학습 속도가 크게 저하되거나 학습이 중지됨 
    - ReLU 활성화 함수를 통해 경사 소실을 방지할 수 있음 
  - 경사 발산
    - 네트워크에서 가중치가 매우 크면 하위 레이어의 경사에 많은 큰 항의 곱이 포함됨
    - 이러한 경우 경사가 너무 커져서 수렴하지 못하고 발산하는 현상이 나타남
    - batch 정규화를 사용하거나 학습률을 낮추면 경사 발산을 방지할 수 있음 
  - ReLU 유닛 소멸
    - ReLU 유닛의 가중 합이 0 미만으로 떨어지면 ReLU 유닛이 고착될 수 있음 
    - 이러한 경우 활동이 출력되지 않으므로 네트워크의 출력에 어떠한 영향도 없음
    - 역전파 과정에서 경사가 더 이상 통과할 수 없음 
    - 경사의 근원이 단절되므로 가중 합이 다시 0 이상으로 상승할 만큼 ReLU가 변화하지 못할 수도 있음 
    - 학습률을 낮추면 ReLU 유닛 소멸을 방지할 수 있음 
- 드롭아웃 정규화
  - 드롭아웃이라는 정규화 형태가 유용함 
  - 단일 경사 스텝에서 유닛 활동을 무작위로 배제하는 방식임 
  - 드롭아웃을 반복할수록 정규화가 강력해짐
    - 0.0 = 드롭아웃 정규화 없음
    - 1.0 = 전체 드롭아웃.모델에서 학습을 수행하지 않음
    - 0.0~1.0 범위 값 = 보다 유용함 


## 프로그래밍 실습  
