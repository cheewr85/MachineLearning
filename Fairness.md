## 공정성
- ML의 일반적인 과정
  - 학습 데이터를 수집하고 주석을 추가 -> 모델을 학습 -> 콘텐츠를 필터링, 집계 또는 생성하거나 콘텐츠의 순위를 지정 -> 사용자에게 출력이 표시
- 학습 데이터를 수집하고 주석을 추가
  - 데이터에서 사람이 갖는 편향
    - 보고 편향
    - 표본 선택 편향
    - 과잉 일반화
    - 외부 집단 동질화 편향
    - 세상 데이터의 양식을 사용할 때 ML에 반영할 수 있는 '세상'의 무의식적 편향
    - 확증 편향
    - 자동화 편향
    - ML에 반영할 수 있는 절차의 무의식적 편향
- 공정성을 위한 설계
  - 1.문제 살펴보기
  - 2.전문가에게 문의
  - 3.모델이 편향을 고려하도록 학습시킴
  - 4.결과 해석
  - 5. 맥락과 함께 게시

### 편향의 유형
- 머신러닝 모델이라고 해서 본질적으로 객관적인 것은 아님/데이터 사전준비와 선정에 사람이 관여하기 때문에 모델의 예측이 편향되기 쉬움
- 일반적인 사람들의 편향을 인식하여 영향을 최소화할 수 있도록 미리 조치하는 것이 중요함 
- 아래의 유형은 모든 편향을 다른 것은 아님/참조:https://en.wikipedia.org/wiki/List_of_cognitive_biases/잠재적인 편향 원인도 생각해야함

- 보고 편향
  - 데이트 세트에 수집된 이벤트, 속성 및 결과의 빈도가 실제 빈도를 정확하게 반영하지 않을 때 나타남
  - 이 편향은 사람들이 '말할 필요도 없다고 느끼는' 일반적인 상황은 언급하지 않고 특별히 기억할 만하거나 특이항 상황만을 기록하려는 경향이 있어 발생
  - 예시
    - 인기 웹사이트에서 사용자가 제출하는 말뭉치를 바탕으로 도서 리뷰가 긍정인지 부정인지 예측하는 모델
    - 도서에 관해 별다른 의견이 없는 사람들은 리뷰를 제출할 가능성이 작기 때문에 리뷰 대다수는 극단적인 의견이 됨 
    - 이 모델은 좀 더 미묘한 어휘를 사용한 도서 리뷰의 감정을 정확히 예측할 가능성이 작음
- 자동화 편향
  - 두 시스템의 오류율과 관계없이 자동화 시스템이 생성한 결과를 비자동화 시스템이 생성한 결과보다 선호하는 경향
  - 예시
    - 톱니바퀴 제조업체에서 근무하는 소프트웨어 엔지니어가 톱니 결함을 파악하도록 학습한 새로운 '혁신적인' 모델을 배포하고 싶어함
    - 공장 감독자는 이 모델의 정밀도와 재현율이 사람 검사자보다 모두 15% 낮다고 지적함 
- 표본 선택 편향
  - 데이터 세트의 사례가 실제 분포를 반영하지 않는 방식으로 선정된 경우 발생함
  - 포함 편향/선택된 데이터가 대표성을 갖지 않음
  - 예시
    - 자사 제품을 구매한 소비자층을 대상으로 전화 설문조사를 하고 그 결과를 토대로 미래의 신제품 판매량을 예측하도록 학습된 모델
    - 경쟁업체 제품을 선택한 소비자는 설문조사 대상이 아니었기 때문에 결과적으로 이 소비자 그룹은 학습 데이터에 반영 안됨
  - 무응답 편향(응답 참여 편향)/데이터 수집 시 참여도의 격차로 인해 데이터가 대표성을 갖지 못함
  - 예시
    - 제품을 구매한 소비자 샘플과 경쟁업체 제품을 구매한 소비자 샘플을 대상으로 한 전화 설문자소를 통해 미래의 신제품 판매량 예측 모델
    - 경쟁업체 제품을 구매한 소비자는 설문조사를 완료하지 않을 가능성이 80% 높았기 때문에 이들의 데이터가 샘플에서 실제보다 작게 표현됨
  - 표본 추출 편향/데이터 수집 과정에서 적절한 무작위선택이 적용되지 않았음
  - 예시
    - 제품을 구매한 소비자 샘플과 경쟁업체 제품을 구매한 소비자 샘플을 대상으로 한 전화 설문조사를 통해 미래의 신제품 판매량을 예측하는 모델
    - 설문에서 소비자를 임의로 타겟팅하지 않고 선착순으로 이메일에 응답한 200명의 소비자를 선정함
    - 평균 구매자보다 제품에 관심이 많은 소비자가 다수 포함되었을 가능성이 높음
- 그룹 귀인 편향
  - 개인의 특성을 개인이 속한 그룹 전체의 특성으로 일반화하려는 경향을 말함
  - 내집단 편향/자신이 소속된 그룹 또는 본인도 공유하는 특성을 가진 그룹의 구성원을 선호하는 경향
  - 예시
    - 소프트웨어 개발자 모집을 위한 이력서 심사 모델을 학습시키는 두 명의 엔지니어는 자신들과 같은 컴퓨터 공학 아카데미에 다녔던 지원자가 더 직무에 적합하다고 믿음 
  - 외부 집단 동질화 편향/자신이 속하지 않은 그룹의 개별 구성원에 관해 고정 관념을 갖거나 그들이 모두 동일한 특징을 가진다고 판단하는 경향
  - 예시
    - 소프트웨어 개발자 모집을 위한 이력서 심사 모델을 학습시키는 두 명의 엔지니어는 자신들과 같은 컴퓨터 공학 아카데미에 다니지 않는 지원자가 직무에 필요한 전문지식이 부족하다고 믿음 
- 내재적 편향
  - 일반적으로 적용할 필요가 없는 자신의 정신적 모델과 개인적 경험을 바탕으로 가정할 때 발생함 
  - 예시
    - 동작 인식 모델을 학습시키는 중인 엔지니어가 '아니요'라는 단어를 나타내는 고개 가로 젖기를 기능으로 사용하려고 함
    - 일부 지역에서 고개를 가로 젓는 것은 반대로 '예'를 의미하기도 함
  - 내재적 편향의 일반적인 형태는 확증 편향으로 모델을 만드는 사람이 자기도 모르게 이미 가지고 있는 믿음이나 가설을 긍정하는 방향으로 데이터를 처리하는 것을 말함 
  - 경우에 따라 모델을 만드는 사람이 자신의 원래 가설과 일치할 때까지 반복해서 모델을 학습시키기도 하는데 이를 실험자 편향이라고 함 
  - 예시
    - 엔지니어가 다양한 특징(키,체중,종,환경)을 바탕으로 개의 공격성을 예측하는 모델을 만들고 있음
    - 이 엔지니어는 어릴 때 활동성이 강한 토이 푸들로 인해 불쾌한 일이 있었음
    - 이후 항상 토이푸들을 공격적인 종이라고 생각
    - 학습된 모델이 대부분의 토이 푸들이 상대적으로 유순하다고 예측했을 때 엔지니어는 크기가 작은 푸들이 더 공격적이라는 결과가 나올 때까지 모델을 여러번 다시 학습시킴 
    
### 편향 식별하기
- 모델에서 데이터를 가장 잘 표현할 방법을 찾기 위해 데이터를 살펴볼 때 공정성 문제를 염두에 두고 편향의 원인이 될 수 있는 요소를 사전에 점검하는 것이 중요 
- 특성 값 누락
  - 데이터 세트의 다수의 예에서 값이 누락된 특성이 하나 이상 있는 경우 데이터 세트의 주요 특성 중 일부가 제대로 표현되지 않았음을 나타내는 지표일 수 있음 
  - 예시
  - 아래의 사진을 통해서 모든 특성의 count가 17000이라는 것은 누락된 값이 없음을 나타냄
  <img src="https://user-images.githubusercontent.com/32586985/71537168-7af71f80-295b-11ea-9982-f6306986117c.PNG">
  
  - 아래의 사진을 보게 된다면 3가지 특성의 count가 3000이라고 가정할 시 각 특성에 14000개의 누락된 값이 있는것
  - 14000개의 값이 누락되어 가구의 평균 소득과 주택 가격의 중앙값을 정확히 연관시키기 훨씬 어려워졌음
  - 이 데이터 모델을 학습하기 전에 누락된 값의 원인을 신중하게 조사하여 소득 및 인구 데이터 누락의 원인이 될 수 있는 잠재적인 편향이 없는지 확인하는 것이 좋음 
  <img src="https://user-images.githubusercontent.com/32586985/71537194-d2958b00-295b-11ea-8e01-6d41f5c75a4c.PNG">
  
- 예기치 않은 특성 값
  - 데이터를 살펴볼 때 특이하거나 비정상적인 특성 값을 포함하는 예가 있는지 확인해 보아야 함
  - 이와 같이 예기치 않는 특성 값이 있다는 것은 데이터 수집 중에 문제가 발생했거나 편향을 일으킬 수 있는 기타 부정확성이 있음을 나타낼 수 있음 
  - 예시
  <img src="https://user-images.githubusercontent.com/32586985/71537216-1c7e7100-295c-11ea-9036-0971e4380c7a.PNG">
  
  - 예기치 않는 특성 값은 4번의 예인데 이 좌표는 미국 캘리포니아 주에 속하지 않음
  <img src="https://user-images.githubusercontent.com/32586985/71537224-4172e400-295c-11ea-8237-882a970567bf.PNG">
  
- 데이터 격차
  - 특정 그룹이나 특성이 실제보다 과소 또는 과대 표현되는 모든 종류의 데이터 격차로 인해 모델에 편향이 생길 수 있음
  - 검증 프로그래밍 실습에서 학습 세트와 검증 세트로 나누기 전에 캘리포니아 주택 데이터 세트를 무작위로 섞지 않아서 확연한 데이터 격차가 생긴것을 알 수 있음 
  - 그림 1은 전체 데이터 세트에서 추출한 데이터의 하위 집합을 캘리포니아 북서부 지역만 나타내도록 시각화한 것임 
  - 캘리포니아주 지도 위에 캘리포니아 주택 데이터 세트의 데이터를 오버레이한 그림
  - 각각의 점은 주택 단지를 나타내며, 파란색은 주택 가격 중앙값이 낮은 곳을, 빨간색은 주택 가격 중앙값이 높은 곳임을 나타냄
  <img src="https://user-images.githubusercontent.com/32586985/71537257-be05c280-295c-11ea-8d1f-3ea21f4b341c.PNG">
  
  - 이와 같이 전체를 잘 대표하지 못하는 샘플을 캘리포니아주 전체의 주택 가격 예측을 위한 모델을 학습하는 데 사용했다면 캘리포니아주 남부의 주택 데이터가 없는것이 문제가 됨
  - 모델에 인코딩된 지리적 편향은 데이터에 표현되지 않은 커뮤니티의 주택 구매자에게 부정적인 영향을 미칠 수 있음 
  
  ### 편향 평가
  - 모델을 평가할 때 전체 테스트 또는 검증세트를 기준으로 계산된 지표가 모델의 공정성에 관해 항상 정확한 모습을 보여주는 것은 아님 
  - 환자의 의료 기록 1000개로 구성된 검증세트가 있다고 가정/그를 바탕으로 종양의 유무를 예측하는 새로운 모델 개발
  - 500개 기록은 여성환자의 기록이고 나머지 500개는 남성 환자의 기록임
  - 다음 혼동행렬은 전체 1000개 사례의 결과를 요약한 것임
  <img src="https://user-images.githubusercontent.com/32586985/71537293-4e440780-295d-11ea-85ae-acb549186ad9.PNG">
  
  - 정밀도가 80%이고 재현율이 72.7%이므로 유먕한 결과로 보임/하지만 각 환자 세트에 관한 결과를 따로 계산한다면?
  - 각 그룹 모델의 성과가 크게 다른 것을 알 수 있음 
  <img src="https://user-images.githubusercontent.com/32586985/71537304-82b7c380-295d-11ea-9eae-4e1bf873e2e0.PNG">
  
  - 여성 환자
    - 실제로 종양이 있었던 여성 환자 11명에 관해 모델은 10명의 환자를 양성으로 정확하게 예측하여 90.9% 재현율을 보임
    - 이 모델은 여성 환자 사례의 9.1%에 관해서는 종양 진단을 놓친 것임
  - 남성 환자
    - 하지만 실제로 종양이 있었던 남성 환자 11명에 관해 모델은 6명의 환자만을 양성으로 정확하게 예측함(재현율 54.5%)
    - 이 모델은 남성 환자 사례의 45.5%에 관해서는 종양 진단을 놓친 것임
    - 모델이 남성 환자의 종양에 양성을 반환할 때 9개 중 6개 사례에서만 정확함(정밀도 66.7%)
    - 다시 말해 이 모델은 남성 환자 사례의 33.3%에 관해서는 종양을 잘못 예측한 것임 


## 프로그래밍 실습

