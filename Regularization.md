## 단순성을 위한 정규화
- 과잉교차
### 작업 1: 주어진 모든 교차곱 특성을 사용하여 모델을 그대로 실행해보자/모델이 데이터 적합성을 갖는 방식에 예외적인 부분이 있는가? 어떤 문제가 있나?
- 모델의 결정 경계는 비정상적으로 보임/데이터에 명시적인 근거가 없음에도 불구하고 왼쪽 상단에 파란색 쪽으로 암시하는 영역이 있음 
- x1과 x2로부터 뻗어 나온 선은 특성 교차에서 나온 선보다 훨씬 두꺼움/특성 교차가 일반(교차하지 않는)특성보다 모델에 기여하는 정도가 적음 
<img src="https://user-images.githubusercontent.com/32586985/70430025-35aea300-1abd-11ea-946e-ad96d37f935c.PNG">

### 작업 2: 다양한 벡터곱 특성을 제거하여(미미할지라도) 성능을 개선해 보아라/왜 특성을 제거하면 성능이 개선되는가?
- 모든 특성 교차를 제거하면 더 정상적인 모델을 얻을 수 있으며(과적합을 암시하는 곡선 경계가 사라짐) 테스트 손실이 수렴하게 됨
- 1000회 반복하고 나면 테스트 손실값은 특성 교차가 영향을 미칠 때보다 약간 낮아짐(데이터 세트에 따라 결과가 다를 수 있음)
- 이 실습에서 하는 데이터는 기본적으로 선형 데이터이며 노이즈가 포함되어 있음
- 너무 복잡한 모델을 사용하면 학습 데이터의 노이즈에 맞추려고 하기 때문에 종종 테스트 데이터에서 모델이 잘 작동하지 않기도 함 
<img src="https://user-images.githubusercontent.com/32586985/70430435-28de7f00-1abe-11ea-9214-17b86c8a0d98.PNG">

## 정규화
- 모델의 복잡도에 패널티를 줌으로써 과적합을 줄이는 것 
- 모델 복잡도에 패널티 부여
  - 가능하면 모델 복잡도를 방지하려고 함 
  - 학습 단계에서 수행하는 최적화에 이 아이디어를 적용할 수 있음 
  - 구조적 위험 최소화
    - 학습 오류를 낮추는 것이 목표
    - 또한 복잡도를 낮출 수 있도록 조정 
    - 최소화:손실(데이터|모델)+복잡도(모델)
- 복잡도(모델)를 정의하는 방법
  - 더 작은 가중치 선호
- 여기에서 벗어나면 비용이 발생함 
- L2 정규화(일명 능선)를 통해 이 아이디어를 구현할 수 있음 
  - 복잡도(모델) = 가중치의 제곱의 합
  - 아주 큰 가중치에 대한 패널티 부여
  - 선형 모델에서는 더 평평한 기울기를 선호
  - 베이지안 사전 확률:
    - 가중치는 0을 중심으로 배치되어야 함
    - 가중치는 정규 분포되어야함 
- L2 정규화를 이용한 손실 함수    
<img src="https://user-images.githubusercontent.com/32586985/70430999-5b3cac00-1abf-11ea-8417-1b6623d39f3d.PNG">

### 단순성을 위한 정규화:L2 정규화
- 다음이 보여주는 일반화 곡선은 학습 반복 횟수에 대해 학습 세트와 검증 세트의 손실을 보여줌 
- 학습 세트와 검증 세트에서의 손실 
<img src="https://user-images.githubusercontent.com/32586985/70431079-8b844a80-1abf-11ea-9e50-5113605b9132.PNG">

- 그림을 본다면 학습 손실은 점차 감소하지만 검증 손실은 결국 증가하는 모델임을 알 수 있음 
- 이 일반화 곡선은 모델이 학습 세트의 데이터에 대해 과적합하다는 것을 보여줌 
- 복잡한 모델에 패널티를 부여하는 정규화라는 원칙을 사용하여 과적합을 방지할 수 있음
- 단순히 손실을 최소화하는 것만을 목표로 삼음(경험적 위험 최소화)
  - 최소화(손실(데이터|모델))
- 구조적 위험 최소화를 통해 다음과 같이 손실과 복잡도를 함께 최소화 할 수 있음
  - 최소화(손실(데이터|모델)+복잡도(모델))
- 이제 학습 최적화 알고리즘은 모델이 데이터에 얼마나 적합한지 측정하는 손실항과 모델 복잡도를 측정하는 정규화 항의 함수가 됨 
  - 지금의 알고리즘과 머신러닝에서 2가지 방법으로 모델 복잡도를 다룸 
    - 모델의 모든 특성의 가중치에 대한 함수로서의 모델 복잡도
    - 0이 아닌 가중치를 사용하는 특성의 총 개수에 대한 함수로서의 모델 복잡도
- 모델 복잡도가 가중치에 대한 함수인 경우 높은 절대값을 사용하는 특성 가중치는 낮은 절대값을 사용하는 특성 가중치보다 더 복잡함 
- 모든 특성 가중치를 제곱한 값의 합계로서 정규화 항을 정의하는 L2 정규화 공식을 사용하여 복잡도를 수치화할 수 있음 
<img src="https://user-images.githubusercontent.com/32586985/70431542-a1463f80-1ac0-11ea-910f-59d7e51ddbee.PNG">

- 위 공식은 0에 가까운 가중치는 모델 복잡도에 거의 영향을 미치지 않는 반면, 이상점 가중치는 큰 영향을 미칠 수 있음 
- 예시
<img src="https://user-images.githubusercontent.com/32586985/70431717-faae6e80-1ac0-11ea-85f7-8230ff48e37d.PNG">
<img src="https://user-images.githubusercontent.com/32586985/70431722-fda95f00-1ac0-11ea-9cf8-aedbaa3123b8.PNG">

- L2 정규화 항은 26.915임 
- 제곱한 값이 25인 굵은 굴씨체로 된 w3는 거의 모든 복잡도에 기여함 
- 다른 5개의 모든 가중치를 제곱한 값의 합계는 L2 정규화 항에 1.1915를 더하기만 하면 됨 

### 단순성을 위한 정규화:람다
- 람다라는 스칼라(정규화율)를 정규화 항의 값에 곱하여 정규화 항의 전반적인 영향을 조정함
- 다음을 수행하는 것을 목표로 함
  - 최소화(손실(데이터|모델)+λ복잡도(모델))
- L2 정규화를 수행하면 모델에 다음과 같은 효과를 줄 수 있음 
  - 가중치 값을 0으로 유도(정확히 0은 아님)
  - 정규(종 모양 또는 가우시안) 분포를 사용하여 가중치 평균을 0으로 유도 
- 람다 값을 높이면 정규화 효과가 강화됨
- 람다 값에 대한 가중치 히스토그램은 밑의 그림처럼 보일 수 있음 
<img src="https://user-images.githubusercontent.com/32586985/70432012-97710c00-1ac1-11ea-8a17-1c25b0c9d630.PNG">

- 람다 값을 낮추면 밑의 그림과 같이 더 평평한 히스토그램이 산출되는 경향이 있음 
<img src="https://user-images.githubusercontent.com/32586985/70432066-ba9bbb80-1ac1-11ea-9a17-70b0ce402ce6.PNG">

- 람다 값을 선택할 때 세워야 할 목표는 단순성과 학습 데이터 적합성 사이에 적절한 균형을 맞추는 것임 
  - 람다 값이 너무 높으면 모델은 단순해지지만 데이터가 과소적합해질 위험이 있음
  - 그렇게 되면 모델은 유용한 예측을 수행할 만큼 학습 데이터에 대해 충분히 학습하지 못할 수 있음 
  - 람다 값이 너무 낮으면 모델은 더 복잡해지고 데이터가 과적합해질 위험이 있음 
  - 모델이 학습 데이터의 특수성을 너무 많이 학습하게 되고 새로운 데이터로 일반화하지 못하게 됨 
- 이상적인 람다 값은 이전에 보지 못했던 새로운 데이터로 효과적으로 일반화되는 모델을 만듬/하지만 이상적인 람다 값은 데이터 의존적이므로 조정 
- 학습률과 람다는 밀접하게 연결되어 있음/강력한 L2 정규화 값은 특성 가중치를 0에 가깝게 유도하는 경향이 있음 
- 낮은 학습률도 종종 같은 효과를 가져옴 이는 0과의 보폭 차이가 그다지 크지 않기 때문임 
- 결과적으로 학습률과 람다를 동시에 변경하면 혼동스러운 효과를 낳을 수 있음 
- 조기중단이란 모델이 완전히 수렴되기 전에 학습을 끝내는 것을 뜻함 
  - 실제로 학습이 온라인(연속적)방식일 경우 일정 부분 암묵적으로 학습을 조기에 중단하는 경우가 많음 
  - 일부 새로운 추세에는 아직 수렴을 위한 데이터가 충분하지 않음 
- 정규화 매개변수 변경으로 인한 효과는 학습률 또는 반복 횟수의 변경으로 인한 효과와의 혼등을 일으킬 수 있음 
  - 한 가지 유용한 방법(고정된 데이터 배치를 가지고 학습하는 경우)은 조기 중단의 영향이 발생하지 않도록 반복 횟수를 충분히 높이는 것임 

### 단순성을 위한 정규화 실습(L2 정규화)

