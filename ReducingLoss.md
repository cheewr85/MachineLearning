# 손실 줄이기
모델을 학습하기 위해서 모델의 손실을 줄이기 위한 방법  

## 손실을 줄이는 방법
- 가중치와 편향에 대한 도함수 (y-y')^2는 주어진 예제의 손실 변화 정도를 보여줌
  - 계산하기 간편하며 볼록 모양 
- 손실을 최소화하는 방향으로 작은 보폭을 반복하여 취함 
  - 기울기 보폭이라고 함(음의 기울기 보폭)
  - 이 최적화 전략을 경사하강법이라고 함  

## 가중치 초기화
- 볼록 문제에서는 가중치가 임의의 값을 가질 수 있음(예: 모두0)
  - 볼록:그릇 모양을 생각하면 됨  
  - 최소값은 단 하나임 
- 예고(foreshadowing): 신경망에서는 해당 없음
  - 볼록하지 않음: 계란판 모양을 생각
  - 최소값이 둘 이상 있음
  - 초기값에 따라 크게 달라짐

## SGD와 미니 배치 경사 하강법
- 각 보폭마다 전체 데이터 세트에 대해 기울기를 계산할 수 있지만 그럴 필요는 없음
- 적은 양의 데이터 샘플에서 기울기 계산이 잘 작동함 
  - 모든 보폭에서 새로운 무작위 샘플을 얻음
- 확률적 경사하강법:한 번에 하나의 예
- 미니 배치 경사하강법:10~1000개의 예로 구성된 배치
  - 손실과 기울기는 배치 전반에 걸쳐 평균 처리됨
  
## 반복 방식
- 반복 학습을 통해 최적 모델을 찾는것 
- 처음에는 임의의 지점에서 시작해서 시스템이 손실 값을 알려줄 때까지 기다림
- 그 이후 다른 값을 추정해서 손실 값을 확인함 
- 최적의 모델을 가능한 한 가장 효율적으로 찾는것 

- 모델을 학습하는 데 사용하는 반복적인 시행착오 과정/반복 방식의 모델 학습 
<img src="https://user-images.githubusercontent.com/32586985/68591019-20048880-04d3-11ea-8c6e-fd410f44a79c.png">

- 반복 전략은 주로 대규모 데이터 세트에 적용하기 용이함  
- 하나 이상의 특성을 입력하여 하나의 예측(y')을 출력하는 모델  
<img src="https://user-images.githubusercontent.com/32586985/68591308-c6e92480-04d3-11ea-9ae1-1494e3856eb0.png">

- 선형 회귀 문제에서는 초기값이 중요하진 않음 임의의 값을 정해도 됨
  - b=0,w1=0
  - 최초 특성 값을 10이라고 가정
  
  ```octave
     y' = 0 + 0(10)
     y' = 0
     % 다음과 같이 출력됨 
  ```     
  
- 위의 다이어그램에서 손실 계산 과정은 이 모델에서 사용할 손실함수임  
- 제곱 손실 함수를 사용한다고 가정한다면 y': 특성 x에 대한 모델의 예측 값/y:특성 x에 대한 올바른 라벨 두개의 입력 값 사용
- 다이어그램의 매개변수 업데이트 계산 과정에 도달/머신러닝 시스템은 손실 함수의 값을 검토하여 b와 w1의 새로운 값을 생성함 
- 새로운 값을 만든 다음 머신러닝 시스템이 이러한 모든 특성을 모든 라벨과 대조하여 재평가하여 손실 함수의 새로운 값을 생성하여 새 매개변수 값을 출력한다고 가정
- 손실 값이 가장 낮은 모델 매개변수를 발견할때까지 반복 학습함
- 전체 손실이 변하지 않거나 매우 느리게 변할 때까지 계속 반복함/이때 모델이 수렴했다고 말함


## 경사하강법  
- w1의 가능한 모든 값에 대해  손실을 계산할 시간과 컴퓨팅 자료가 있다고 가정  
- 손실과 w1을 대응한 도표는 항상 블록 함수 모양을 함 
- 회귀 문제에서는 볼록 함수 모양의 손실 대 가중치 도표가 산출됨  
<img src="https://user-images.githubusercontent.com/32586985/68592031-7a9ee400-04d5-11ea-9bd5-58bec6f7c5b3.png">

- 볼록 문제에는 기울기가 정확하게 0인 지점인 최소값이 하나 존재함/이 최소값에서 손실 함수가 수렴함  
- 전체 데이터 세트에 대해 모든 손실 함수를 계산하는 것은 수렴 지점을 찾는데 비효율적인 방법
- 더 효율적인 방법인 경사하강법이 있음

- 경사하강법의 첫 번째 단계는 w1에 대한 시작 값(시작점)을 선택하는 것 
- 시작점은 별로 중요하지 않음/w1을 0으로 설정하거나 임의의 값을 선택함  
<img src="https://user-images.githubusercontent.com/32586985/68592295-09136580-04d6-11ea-9bad-7963ff173e28.png">

- 시작점에서 손실 곡선의 기울기를 계산함
- 기울기는 편미분의 벡터로서 어느 방향이 더 정확한지 혹은 더 부정확한지 알려줌 
- 단일 가중치에 대한 손실의 기울기는 미분값과 같음  
- 기울기는 벡터이므로 방향과 크기의 특성을 모두 가지고 있음  
- 기울기는 항상 손실 함수 값이 가장 크게 증가하는 방향을 향함
- 경사하강법 알고리즘은 가능한 한 빨리 손실을 줄이기 위해 기울기 반대 방향으로 이동함
- 경사하강법은 음의 기울기를 사용함  
<img src="https://user-images.githubusercontent.com/32586985/68592518-a1114f00-04d6-11ea-95ae-53f8413b6749.png">

- 손실 함수 곡선의 다음 지점을 결정하기 위해 기울기의 크기의 일부를 시작점에 더함 
- 기울기 보폭을 통해 손실 곡선의 다음 지점으로 이동함  
<img src="https://user-images.githubusercontent.com/32586985/68592748-272d9580-04d7-11ea-8021-b5f47d1fa189.png">

- 이 과정을 반복해 최소값에 점점 접근함 


## 학습률  
- 기울기 벡터는 방향과 크기를 모두 가짐
- 경사하강법 알고리즘은 기울기에 학습률 또는 보폭이라 불리는 스칼라를 곱하여 다음지점을 결정함  
- 초매개변수는 머신러닝 알고리즘에서 조정하는 값
- 학습률을 너무 작게 설정하면 학습시간이 매우 오래 걸림 
<img src="https://user-images.githubusercontent.com/32586985/68723979-9bfaef80-05fd-11ea-9466-870435edb303.PNG">

- 학습률을 너무 크게 설정하면 다음 지점이 곡선의 최저점을 무질서하게 이탈할 우려가 있음   
<img src="https://user-images.githubusercontent.com/32586985/68724052-ccdb2480-05fd-11ea-840a-3dffd2173ac2.PNG">

- 모든 회귀 문제에는 골디락스 학습률이 존재함(골디락스: 현재 개념에서는 너무 작지도 너무 크지도 않은 적당한 학습률)
- 골디락스 값은 손실 함수가 얼마나 평탄한지 여부와 관련 있음 
- 손실 함수의 기울기가 작다면 더 큰 학습률을 시도해 볼 수 있음/그러면 작은 기울기를 보완하고 더 큰 보폭을 만들어 낼 수 있음 
- 적절한 학습률
<img src="https://user-images.githubusercontent.com/32586985/68724359-95b94300-05fe-11ea-87de-5ac1546ed9a3.PNG">


## 학습률 최적화  
- 학습률이 작은 경우
<img src="https://user-images.githubusercontent.com/32586985/68724469-f21c6280-05fe-11ea-959e-06f83cb2676a.PNG">

- 학습률이 큰 경우
<img src="https://user-images.githubusercontent.com/32586985/68724544-3ad41b80-05ff-11ea-8555-bc326cc17d43.PNG">

- 학습률이 적절한 경우
<img src="https://user-images.githubusercontent.com/32586985/68724653-7838a900-05ff-11ea-98da-175117e7c450.PNG">
<img src="https://user-images.githubusercontent.com/32586985/68724711-99999500-05ff-11ea-9a15-5912df6d9554.PNG">


## 확률적 경사하강법    
- 경사하강법에서 배치는 단일 반복에서 기울기를 계산하는 데 사용하는 예의 총 개수/배치가 전체 데이터 세트라고 가정함 
- Google 규모의 작업에서는 데이터 세트에 수십억 수천억 개의 예가 포함되는 경우가 많음
    - 또한 엄청나게 많은 특성이 포함되어 있음/배치가 거대해질 수 있음/배치가 너무 커지면 단일 반복으로도 계산하는 데 오랜 시간이 걸림 
    - 대량 데이터 세트에는 중복 데이터가 포함되어 있을 수 있음/중복의 가능성도 높아짐/예측성이 훨씬 높은 값이 대용량 배치에 비해 덜 포함됨
- 데이터 세트에서 예를 무작위로 선택시 훨씬 적은 데이터 세트로 중요한 평균값을 추정할 수 있음 
- 확률적 경사하강법은 이 아이디어를 더욱 확장한 것 
    - 반복당 하나의 예만을 사용함 
    - 반복이 충분하면 효과는 있지만 노이즈가 매우 심함
    - 확률적(Stochastic)이라는 용어는 각 배치를 포함하는 하나의 예가 무작위로 선택된다는 것을 나타냄
- 미니 배치 확률적 경사하강법은 전체 배치 반복과 SGD간의 절충안
    - 미니 배치는 무작위로 선택한 10개에서 1000개 사이의 예로 구성됨  
    - SGD의 노이즈를 줄이면서도 전체 배치보다는 더 효율적임  
 
 
 ## 학습률 및 수렴  
 ### 플레이그라운드 실습 
 - 데이터 세트 생성함/라벨에는 두 가지 값이 가능함(ex/스팸 대 스팸 아님, 건강한 나무 대 병든 나무)
 - 다양한 초매개변수를 조정하여 각 라벨 값을 서로 올바르게 분류(분리 또는 구분)하는 모델을 만드는 것을 목표로 함 
 - 일정 분량의 노이즈를 포함하므로 모든 예를 올바르게 분류하는 것은 불가능함 
 - 모델에 어떠한 영향을 주는가/시각적으로 얼마나 급격히 변화하는가/모델이 수렴한 것처럼 보인후에도 불안정성이 나타날 수 있음 
 - x1 및 x2에서 모델 시각화로 이어지는 선을 확인/이 선의 두께는 모델에서 해당 특성의 가중치를 나타냄/선이 두꺼우면 가중치가 높은 것 
 - 학습률이 3일 경우
 <img src="https://user-images.githubusercontent.com/32586985/68726129-debfc600-0603-11ea-9051-c07ee5b5a27e.PNG">
 
 - 학습률이 3인 경우에는 시각적으로 급격히 변화를 하며 수렴할 때까지 불안정성이 보였음 
 - 모델에서 특성에 대한 가중치가 높음 
 
 
 
 
 - 학습률이 0.1일 경우
 <img src="https://user-images.githubusercontent.com/32586985/68726249-365e3180-0604-11ea-8075-c76e61b137ae.PNG">
 
 - 학습률이 0.1인 경우에는 시각적으로 급격한 변화나 수렴하는 구간까지 짧은 시간이 걸리고 불안정성이 보이지 않음 
 - 모델에서 특성에 대한 가중치가 학습률 3에 비해 낮음 
 
